{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SakaGloria/FMMLLABS_AND_PROJECTS/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32875b39-83e4-40b9-e650-e032850ca75c"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2df057-d586-4d3d-962c-738a88a684cd"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a768db17-9222-46e9-a031-2ea650931afc"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb3828e5-5c36-4f93-801c-ef6f9b2ca67f"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52fa5892-ea3e-4856-f650-bf5f2883e3c3"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92ca549-ffad-4284-de13-af30c301cf7f"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fec159-99b0-49ac-99a0-faeac24cf398"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the training and validation sets can influence how well the model's performance on the validation set predicts its performance on the unseen test set. Here are some considerations:\n",
        "\n",
        "### **1. Sufficient Data for Learning:**\n",
        "   - **Training Set Size:**\n",
        "     - A larger training set generally allows the model to learn more robust and generalized patterns from the data.\n",
        "     - If the training set is too small, the model may struggle to capture the underlying patterns, leading to poor generalization.\n",
        "\n",
        "   - **Validation Set Size:**\n",
        "     - A sufficiently large validation set is crucial for accurately estimating the model's performance. It should be representative of the data distribution and diverse enough to provide a reliable evaluation.\n",
        "\n",
        "### **2. Overfitting and Underfitting:**\n",
        "   - **Overfitting:**\n",
        "     - If the model performs exceptionally well on a small validation set, it might be overfitting to the validation data, capturing noise instead of true patterns. This can result in poor generalization to the test set.\n",
        "\n",
        "   - **Underfitting:**\n",
        "     - Conversely, if the model performs poorly on the validation set, it may not have learned the underlying patterns well, indicating potential underfitting. In such cases, performance on the test set may also be suboptimal.\n",
        "\n",
        "### **3. Generalization Estimate:**\n",
        "   - **Validation Set as a Proxy for Test Set:**\n",
        "     - The validation set is typically used as a proxy for the test set during model development. A good performance on the validation set suggests that the model is likely to generalize well to unseen data.\n",
        "\n",
        "   - **Caution with Small Validation Sets:**\n",
        "     - If the validation set is small, the estimate of model performance may be less reliable. The risk of overfitting to the specific characteristics of the validation set increases.\n",
        "\n",
        "### **4. Cross-Validation:**\n",
        "   - **Using Multiple Validation Sets:**\n",
        "     - Cross-validation, which involves splitting the data into multiple train-validation sets, can provide a more robust estimate of the model's generalization performance.\n",
        "     - It helps mitigate the impact of variability in a single validation set and provides a more reliable indication of how well the model is likely to perform on unseen data.\n",
        "\n",
        "### **5. Model Complexity and Training Duration:**\n",
        "   - **Complex Models:**\n",
        "     - More complex models may require larger datasets for effective training. If the model is too complex for the available data, it may struggle to generalize.\n",
        "\n",
        "   - **Training Duration:**\n",
        "     - The time required to train the model can also be a factor. If training takes a long time, a smaller dataset might limit the number of iterations, potentially impacting convergence.\n",
        "\n",
        "### **Conclusion:**\n",
        "In summary, having sufficiently large and representative training and validation sets is essential for building models that generalize well to unseen data. The validation set's role is to provide a reliable estimate of how well the model is likely to perform on the test set. Cross-validation can enhance the robustness of this estimate, especially when dealing with limited data. Balancing the sizes of the training and validation sets while considering the model's complexity and the nature of the problem is crucial for achieving reliable generalization."
      ],
      "metadata": {
        "id": "vlPXqhvnq3rF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the validation set can have an impact on the accuracy estimate and generalization performance of your machine learning model. Here's how increasing or decreasing the percentage of the validation set may affect your results:\n",
        "\n",
        "### Increasing the Percentage of Validation Set:\n",
        "\n",
        "1. **Advantages:**\n",
        "   - **More Robust Evaluation:** A larger validation set provides a more robust evaluation of the model's performance. It is less susceptible to variability introduced by a small sample of validation data.\n",
        "\n",
        "   - **Better Generalization Estimate:** With more validation samples, the estimate of how well the model generalizes to new, unseen data tends to be more reliable.\n",
        "\n",
        "2. **Considerations:**\n",
        "   - **Reduced Training Data:** Allocating a larger percentage to the validation set means less data available for training. If the training set becomes too small, the model might not learn robust patterns, and overfitting to the training set becomes a concern.\n",
        "\n",
        "   - **Computational Cost:** Training on a smaller training set may lead to faster training times, but it might also limit the model's ability to capture the complexity of the underlying patterns.\n",
        "\n",
        "### Decreasing the Percentage of Validation Set:\n",
        "\n",
        "1. **Advantages:**\n",
        "   - **More Training Data:** Allocating a smaller percentage to the validation set allows more data to be used for training. This can be advantageous, especially with limited data, as it helps the model to learn better representations.\n",
        "\n",
        "   - **Computational Efficiency:** With a larger training set, training times may be shorter, and the model may still achieve good generalization if the data is representative.\n",
        "\n",
        "2. **Considerations:**\n",
        "   - **Less Reliable Evaluation:** A smaller validation set may result in a less reliable estimate of the model's performance. The estimate becomes more susceptible to the specific characteristics of the validation samples.\n",
        "\n",
        "   - **Risk of Overfitting:** With a smaller validation set, there is an increased risk of overfitting to the validation set, and the estimate of generalization performance may be overly optimistic.\n",
        "\n",
        "### Finding the Right Balance:\n",
        "\n",
        "The key is to find a balance between having a sufficiently large validation set for reliable performance estimation and having enough training data for the model to learn effectively. Cross-validation can be a valuable technique to mitigate the impact of variability in the validation set by repeatedly splitting the data into training and validation sets.\n",
        "\n",
        "In practice, the optimal size of the validation set depends on the size and nature of the overall dataset, the complexity of the model, and the specific problem at hand. It often involves a trade-off between reliable performance estimation and effective model training."
      ],
      "metadata": {
        "id": "4lHbR7tVqxLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the number of iterations may help to some extent when dealing with a very small training dataset, but it comes with certain considerations and limitations.\n",
        "\n",
        "### Advantages of Increasing Iterations with a Small Training Dataset:\n",
        "\n",
        "1. **More Exposure to Data:**\n",
        "   - With a small dataset, each iteration allows the model to see the training examples again, potentially reinforcing patterns and improving learning.\n",
        "\n",
        "2. **Improved Convergence:**\n",
        "   - A small dataset might converge more quickly, and increasing iterations can allow the model to continue refining its parameters.\n",
        "\n",
        "### Considerations and Limitations:\n",
        "\n",
        "1. **Overfitting Risk:**\n",
        "   - Increasing iterations with a small dataset raises the risk of overfitting. The model may memorize the training examples rather than learning the underlying patterns.\n",
        "\n",
        "2. **Generalization Concerns:**\n",
        "   - The primary goal in machine learning is to build models that generalize well to new, unseen data. If the dataset is very small, the model might not learn robust representations, and overfitting becomes a significant concern.\n",
        "\n",
        "3. **Data Augmentation and Regularization:**\n",
        "   - Instead of relying solely on increasing iterations, it's often more effective to use techniques like data augmentation (if applicable) and regularization to enhance model generalization.\n",
        "\n",
        "4. **Model Complexity:**\n",
        "   - The model's complexity (e.g., the number of parameters) should be carefully chosen. A complex model may overfit small datasets more easily, and increasing iterations may exacerbate this issue.\n",
        "\n",
        "5. **Validation Dataset Size:**\n",
        "   - If the validation dataset is also small, the estimate of model performance may be less reliable. Cross-validation with small datasets becomes challenging, and the risk of overfitting to a specific split increases.\n",
        "\n",
        "In conclusion, while increasing iterations can be a strategy to extract more information from a small dataset, it should be approached with caution. Combining it with proper regularization, model complexity control, and other techniques (such as data augmentation) is essential to prevent overfitting and improve generalization. Additionally, the overall effectiveness depends on the specific characteristics of the dataset and the nature of the problem being addressed."
      ],
      "metadata": {
        "id": "t_KoXjHfqiJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of iterations, or epochs, in the context of training a machine learning model, can impact the performance estimate. The effect of the number of iterations on the estimate depends on various factors, and there is no one-size-fits-all answer. Here are some considerations:\n",
        "\n",
        "1. **Underfitting and Convergence:**\n",
        "   - If your model is underfitting (i.e., it hasn't learned the underlying patterns in the data), increasing the number of iterations may improve performance up to a certain point.\n",
        "   - Training for too few iterations might result in a model that hasn't converged to a good solution, leading to suboptimal performance.\n",
        "\n",
        "2. **Overfitting:**\n",
        "   - On the other hand, if you train for too many iterations, the model may start to overfit the training data. It might memorize the training set instead of learning the underlying patterns, leading to poor generalization to new, unseen data.\n",
        "   - Regularization techniques (like dropout or L2 regularization) can help mitigate overfitting to some extent.\n",
        "\n",
        "3. **Computational Resources:**\n",
        "   - The number of iterations also depends on computational resources. Training for a large number of iterations may be computationally expensive, and there might be diminishing returns in terms of model improvement.\n",
        "\n",
        "4. **Early Stopping:**\n",
        "   - To find a good balance, practitioners often use techniques like early stopping. This involves monitoring the performance on a validation set during training and stopping the training process when performance starts to degrade (indicating potential overfitting).\n",
        "\n",
        "5. **Learning Rate:**\n",
        "   - The learning rate, which determines the size of the steps taken during optimization, can interact with the number of iterations. Too high of a learning rate may lead to overshooting optimal parameter values, while too low of a learning rate may result in slow convergence.\n",
        "\n",
        "In summary, the relationship between the number of iterations and the performance estimate is complex. It involves finding a balance between underfitting and overfitting, considering computational constraints, and often involves hyperparameter tuning. Cross-validation can help in assessing the impact of different numbers of iterations on model performance across multiple folds of the data, providing insights into the generalization ability of the model."
      ],
      "metadata": {
        "id": "r-80R8FZqSXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation, by providing a more robust estimate of model performance on the validation set, can give a more accurate estimate of how well a model is likely to perform on unseen data, such as a test set. However, it's important to note that the test accuracy is not directly estimated by cross-validation.\n",
        "\n",
        "In a typical machine learning workflow, you use cross-validation on your training data to tune hyperparameters, select models, and assess performance. Once you've finalized your model based on these evaluations, you then evaluate it on a separate and previously unseen test set. The test set is crucial for providing an unbiased and final estimate of the model's generalization performance.\n",
        "\n",
        "Cross-validation helps in providing a more reliable estimate of how well your model generalizes to different subsets of the training data. This can give you confidence that your model is not overfitting to a particular training-validation split. However, the final measure of accuracy or performance should come from evaluating the model on the test set.\n",
        "\n",
        "In summary, cross-validation aids in model selection and hyperparameter tuning, leading to a more reliable and generalizable model. But the ultimate measure of accuracy on unseen data is obtained by evaluating the model on a separate test set that was not used during training or cross-validation."
      ],
      "metadata": {
        "id": "yT-gXgEEqJld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, averaging validation accuracy across multiple splits, often referred to as cross-validation, can provide more consistent and reliable results compared to a single split. Cross-validation is a technique used in machine learning to assess the performance of a model by splitting the dataset into multiple subsets (folds). The model is trained on several combinations of training and validation sets, and the performance metrics are averaged across these folds.\n",
        "\n",
        "The main advantages of cross-validation include:\n",
        "\n",
        "1. **Reduced Variance:** Averaging results over multiple folds helps reduce the variance in performance estimation. This is particularly important when the dataset is limited, and a single split may not be representative of the overall model performance.\n",
        "\n",
        "2. **Better Generalization:** Cross-validation provides a more robust estimate of how well a model generalizes to unseen data. By training and validating on different subsets of the data, the model is exposed to a variety of patterns and variations present in the dataset.\n",
        "\n",
        "3. **Mitigating Overfitting or Selection Bias:** Averaging results over multiple folds helps mitigate the risk of overfitting to a specific subset of the data. It also reduces the chance of accidentally selecting a biased subset that might not be representative of the overall dataset.\n",
        "\n",
        "4. **More Reliable Performance Metrics:** The variability in model performance metrics is better captured through cross-validation, leading to more reliable and stable estimates of accuracy, precision, recall, etc.\n",
        "\n",
        "However, it's essential to note that the choice of the number of folds can impact the trade-off between computational cost and the reliability of the results. Common choices include 5-fold or 10-fold cross-validation, but the optimal number may depend on the size of the dataset and the specific problem at hand.\n",
        "\n",
        "In summary, averaging validation accuracy across multiple splits through cross-validation is generally a good practice for obtaining more consistent and reliable performance estimates for machine learning models."
      ],
      "metadata": {
        "id": "mfedkrGTp9Hl"
      }
    }
  ]
}